# Reinforcement Learning for Visual Comprehension
*A policy-gradient approach to learning visual attention strategies with memory-based context encoding.*
# Driving Question
If we give a Reinforcement Learning agent 2 tools: **vision** and **memory** will it be able to scan an image and learn "where to look" 


# Reinforcement Learning:
### 1. Action Space
The action space is defined as such: For a **h** x **w** the action space is each point on the image with a stride of **s** the action space is defined as follows:

image size: $I$ \
patch size: $P$ \
stride: $s$

$$
N = \frac{(I - P)}s\ + 1
$$

Each possible action is a $(x,y)$ coordinate where $x,y \in [0, 1]$ and is sampled from the probability distribution generated by the policy over the action space.  
### 2. Reward
To avoid a sparse reward signal I used two rewards:
1. Final Classification Reward
    $$2 \text{ if } \hat{y} = y \text{ else } -2$$
2. Per Step Confidence Rewards
$$
\Delta \text{CE} \coloneqq CE_i - CE_{i-1}
$$
with the clamp:
$$
-0.1 < \Delta \text{CE} < 0.1
$$
This encourages the model to take steps that maximize information gain.
### 3. Updating
This policy is trained using REINFORCE with a baseline to improve learning stability

# Classification:
1. Using the policy generate an action based on the previous state; $(x,y)$ center position to extract a patch from.

2. Pass this patch into an encoder (a small CNN or MLP)

3. Pass all current embeddings $e_0-e_t$ into the final (Attention + RNN) + Classifier head block to get current class confidences and compute per step $\Delta CE$ reward

3. Pass this patch embedding into the agents memory (A LSTM) and generate the context vector for the next state

4. Repeat (1-3) for a pre-specified number of steps

5. Concat embeddings $e_0-e_n$ and pass through a (Attention + RNN) -> Classifier to get final class predictions


# Experiments
The following commands show how to run a few experiments

```bash
python main.py --pretrain True --patch_size 8 --steps 6 --img_size 64 --clutter_count 4 --stride 4
```
This will run on cluttered MNIST with the above parameters. The vision sequence can be seen below. The agent can be seen first locating the number in the image. then with the remaining steps looking at the number.

| ![](gifs/gifs_experiment1/attention1.gif) | ![](gifs/gifs_experiment1/attention2.gif) | ![](gifs/gifs_experiment1/attention3.gif) |
|--------------------------------------|--------------------------------------|--------------------------------------|


| ![](gifs/gifs_experiment1/attention4.gif) | ![](gifs/gifs_experiment1/attention5.gif) | ![](gifs/gifs_experiment1/attention6.gif) |
|--------------------------------------|--------------------------------------|--------------------------------------|

```bash
python main.py --pretrain True --patch_size 3 --steps 8 --img_size 28 --clutter_count 0 --stride 3
```
This will run on normal MNIST as the image size specified is the default size for MNIST. The agent can be seen taking glimpses at and around the number. 

| ![](gifs/gifs_experiment2/attention1.gif) | ![](gifs/gifs_experiment2/attention2.gif) | ![](gifs/gifs_experiment2/attention3.gif) |
|--------------------------------------|--------------------------------------|--------------------------------------|


| ![](gifs/gifs_experiment2/attention4.gif) | ![](gifs/gifs_experiment2/attention5.gif) | ![](gifs/gifs_experiment2/attention6.gif) |
|--------------------------------------|--------------------------------------|--------------------------------------|

# References
* https://github.com/bentrevett/recurrent-attention-model
* https://arxiv.org/pdf/1406.6247